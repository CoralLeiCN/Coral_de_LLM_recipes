{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Related codes in \n",
    "https://www.kaggle.com/whitepaper-foundational-llm-and-text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exercise 1: call LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for **Large Language Model**.  It's a type of artificial intelligence (AI) model that's been trained on a massive amount of text data to understand and generate human-like text.  This allows LLMs to perform a wide range of tasks, including:\n",
      "\n",
      "* **Text generation:** Creating different forms of text, like articles, poems, code, scripts, and emails.\n",
      "* **Translation:** Converting text from one language to another.\n",
      "* **Question answering:** Responding to questions in a comprehensive and informative way.\n",
      "* **Summarization:** Condensing large amounts of text into shorter summaries.\n",
      "* **Text classification:** Categorizing text into different groups or topics.\n",
      "* **Sentiment analysis:** Determining the emotional tone of text.\n",
      "\n",
      "Essentially, LLMs are sophisticated algorithms that have learned the patterns and relationships between words and phrases in language, enabling them to produce and understand human-like text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# google AI Studio SDK\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# update with your API key\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "# choose the model\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash-8b\")\n",
    "response = model.generate_content(\"What is LLM?\")  # set your prompt here\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "import time\n",
    "\n",
    "base_model = \"models/gemini-1.5-flash-001-tuning\"\n",
    "training_data = [\n",
    "    {\"text_input\": \"1\", \"output\": \"2\"},\n",
    "    # ... more examples ...\n",
    "    # ...\n",
    "    {\"text_input\": \"seven\", \"output\": \"eight\"},\n",
    "]\n",
    "operation = genai.create_tuned_model(\n",
    "    # You can use a tuned model here too. Set `source_model=\"tunedModels/...\"`\n",
    "    display_name=\"increment\",\n",
    "    source_model=base_model,\n",
    "    epoch_count=20,\n",
    "    batch_size=4,\n",
    "    learning_rate=0.001,\n",
    "    training_data=training_data,\n",
    ")\n",
    "\n",
    "for status in operation.wait_bar():\n",
    "    time.sleep(10)\n",
    "\n",
    "result = operation.result()\n",
    "print(result)\n",
    "# # You can plot the loss curve with:\n",
    "# snapshots = pd.DataFrame(result.tuning_task.snapshots)\n",
    "# sns.lineplot(data=snapshots, x='epoch', y='mean_loss')\n",
    "\n",
    "model = genai.GenerativeModel(model_name=result.name)\n",
    "result = model.generate_content(\"III\")\n",
    "print(result.text)  # IV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
