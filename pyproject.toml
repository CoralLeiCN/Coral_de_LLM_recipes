[project]
name = "coral-de-llm-recipes"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12, <3.13"
dependencies = [
 "bitsandbytes>=0.45.3",
 "datasets>=3.3.2",
 "ipywidgets>=8.1.5",
 "jupyter>=1.1.1",
 "langchain-core>=0.3.36",
 "langchain-google-genai>=2.0.9",
 "langgraph>=0.2.73",
 "peft>=0.14.0",
 "tensorboardx>=2.6.2.2",
 "transformers>=4.48.3",
 "trl>=0.15.1",
 "opentelemetry-sdk>=1.30.0",
 "opentelemetry-exporter-otlp>=1.30.0",
 "openinference-instrumentation-smolagents>=0.1.6",
 "google-genai>=1.5.0",
 "litellm>=1.63.3",
 "gradio-client>=1.7.2",
 "torch>=2.5.1",
 "torchvision>=0.20.1",
 "google-search-results>=2.4.2",
 "langchain>=0.3.21",
 "langchain-community>=0.3.20",
 "rank-bm25>=0.2.2",
 "plotly>=6.0.1",
 "geopandas>=1.0.1",
 "shapely>=2.0.7",
 "kaleido==1.0.0rc0",
 "helium>=5.1.1",
 "selenium>=4.30.0",
 "python-dotenv>=1.0.1",
 "chromadb>=0.6.3",
 "llama-index>=0.12.25",
 "llama-index-vector-stores-postgres>=0.4.2",
 "llama-index-vector-stores-chroma>=0.4.1",
 "llama-index-embeddings-google-genai>=0.1.0",
 "llama-index-llms-google-genai>=0.1.6",
 "llama-index-callbacks-arize-phoenix>=0.4.0",
]


[dependency-groups]
dev = ["ipykernel>=6.29.5"]


#https://pytorch.org/get-started/locally/
[[tool.uv.index]]
name = "pytorch-cu124"
url = "https://download.pytorch.org/whl/cu124"
explicit = true

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true
